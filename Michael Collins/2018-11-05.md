天气：晴  
阅读时间：早班车


# chapter 1
+ language model  
lagnguage models were originally developed for the problem of speech recognition.

why we need language models?  
Language models is very useful as a good 'prior' distribution over which sentences are or aren't probable in language.  

+ strengths and weaknesses of trigram language models (maximum-likelihood estimates)  
solution to data sparsity (smoothed estimation methods):
The key idea will be to rely on lower-order statistical estimates (using estimates based on bigram or unigram counts to smooth the estimates based on trigrams).
1). linear interpolation
吴军-两个语言模型
q(w|u,v) = lambda1 * q(w|u,v) + lambda2 * q(w|v) + lambda3 * q(w)
extension:
Add an additional degree of freedom, by allowing the values of parameters to vary depending on the bigram that is being conditioned on.  
(Intuition, if the q(w|u,v) is more confident, then give more weights to it!)  
a). bucketing  

> question:  
why the parameters of linear interpolation should be estimated by development data, rather than training data?  

2). discounting methods    


+ perplexity
under a uniform probability model, the perplexity is equal to the vocabulary size.  


+ summary


+ assignment

