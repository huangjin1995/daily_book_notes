天气：晴  
阅读时间：早班车


# chapter 1
+ language model  

  what's original of language model?  

  language models were originally developed for the problem of speech recognition.

  why we need language models?  
  Language models is very useful as a good 'prior' distribution over which sentences are or aren't probable in language.  

  what's trigram language model?

  $p(x_1,...,x_n)=\prod_{i=1}^n q(x_i|x_{i-2},x_{i-1})$

+ strengths and weaknesses of trigram language models (maximum-likelihood estimates)  
  solution to data sparsity (smoothed estimation methods):  
  The key idea will be to rely on lower-order statistical estimates (using estimates based on bigram or unigram counts to smooth the estimates based on trigrams).  

    1. linear interpolation  

    $$q(w|u,v)=\lambda_1 \times q_{ML}(w|u,v) + \lambda_2 \times q_{ML}(w|v) + \lambda_3 \times q_{ML}(w)$$

    extension:  
    Add an additional degree of freedom, by allowing the values of parameters to vary depending on the bigram that is being conditioned on.  
    (Intuition, if the q(w|u,v) is more confident, then give more weights to it!)  
    a). linear interpolation with bucketing(page17)  
    define **the bucket function(like histogram)** for the bigrams count, for each bucket using its own set of smoothing paramters.  
    b). simpler method  
    $$\lambda_1 = \frac{c(u,v)}{c(u,v)+\gamma}$$

    $$\lambda_2=(1-\lambda_1) \times \frac{c(v)}{c(v)+\gamma}$$

    $$\lambda_3=1-\lambda_1-\lambda_2$$

    The only parameter is the $\gamma$.  

    2. discounting methods  
      It reflects the intuition that if we take counts from the training corpus, we will systematically over-estimate the probability of bigrams seen in the corpus (and under-estimate bigrams not seen in the corpus).  
      For the bigrams count larger than zero part, we discount the bigrams count, and it leads to the missing probability mass.  
      For the bigrams count identical to zero part, we using the missing probability mass as weight average of the probability of unigrams.  
      The only parameter is the discounting value.  

> question:  
why the parameters of linear interpolation should be estimated by development data, rather than training data?  


+ perplexity  
under a uniform probability model, the perplexity is equal to the vocabulary size.  


+ summary


+ assignment





