天气：晴  
阅读时间：早班车

# chapter 3

### Context-Free Grammars

$N$ is a finite set of non-terminal symbols.

$\Sigma$ is a finite set of terminal symbols.

$R$ is a finite set of rules of the form $X \to Y_1Y_2...Y_n$, where $X \in N$, $n \geq 0$, and $Y_i \in (N  \cup \Sigma)$ for $i=1...n$.

...

### Probabilistic Context-Free Grammars(PCFGs)

Probabilistic Context-Free Grammars(PCFGs), also known as the Stochastic Context-Free Grammars(SCFGs), first proposed by Booth(1969).

### Structure Ambiguity (Lexical)

It is not uncommon for a moderate-length sentence (say 20 or 30 words in length) to have hundreds, thousands, or even tens of thousands of possible parses. <br>**Structural ambiguity**, which arises from many commonly used rules in phrase-structure grammars. Structural ambiguity comes in many forms.Two common kinds of ambiguity are **attachment ambiguity** (such as, PP-attachment), **coordination ambiguity** (besides, noun-phrase bracketing ambiguity), **sub-categorization ambiguity**.<br>But CFGs don't model syntactic facts about specific words.

### Structure Dependencies

PCFGs can be applied both to disambiguation in syntactic parsing and to word prediction in language modeling (Special useful in long-distance dependencies).<br>But CFGs impose an independence assumption on probabilities, resulting in poor modeling of structural dependencies across the parser tree.

### The CKY algorithm

The CKY (Cocke-Kasami (1965)-Younger (1967)) algorithm is a dynamic-programming algorithm.

> question: what's difference between viterbi algorithm and cky algorithm?

### The probabilistic CKY algorithm

The probabilistic CKY algorithm first described by Ney (1991).

Where do PCFG rule probabilities come from? <br>There are two ways to learn probabilities for the rules of a grammar. <br>The simplest way is to use a treebank, a corpus of already parsed sentences.<br>The second way is to parsing a corpus of sentences with a (non-probabilities) parser, if sentences were unambiguous...but....The intuition for solving this  chicken-and-egg problem is to incrementally improve our estimates by beginning with a parser with equal rule probabilities, then parser the sentence...re-estimate the rule probabilities, and so on, until our probabilities converge. This solution is called the inside-outside algorithm, it was proposed by Baker (1979) as a generalization of **the forward-backward algorithm for HMMs**. It is **a special case of the Expectation Maximization (EM) algorithm**.





==Notes==: This chapter is very difficult to understand especially for the algorithm! But the example is very clear!















