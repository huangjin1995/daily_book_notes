Fixup Initialization: Residual Learning Without Normalization

http://export.arxiv.org/abs/1901.09321



refer: CS231n, http://cs231n.github.io/neural-networks-1/



### Question

what happens when W=0 init is used?

all the neurons will do the same thing



### Strategy

- First idea: Small random numbers
  (gaussian with zero mean and 1e-2 standard deviation)

  Works ~okay for small networks, but problems with deeper networks.

- 



### Debug 

+ 



+ 









